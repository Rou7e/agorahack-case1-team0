{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rusla\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "import pymorphy2 \n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "#import tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, MaxPooling1D, Conv1D, GlobalMaxPooling1D, Dropout, LSTM, GRU\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib online "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = './'\n",
    "\n",
    "with open(DIR + 'agora_hack_products/agora_hack_products.json', encoding='utf-8') as f:\n",
    "   prdct = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(prdct, orient='columns')\n",
    "\n",
    "labels = df[df['is_reference'] == True]['product_id'].count()\n",
    "\n",
    "df.loc[(df['is_reference'] == True),'reference_id'] = df['product_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>name</th>\n",
       "      <th>props</th>\n",
       "      <th>is_reference</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>4ce12b22a6dfed7a</td>\n",
       "      <td>Mi Robot Vacuum-Mop 2 Pro EU White</td>\n",
       "      <td>[Тип   контейнера\\tдля пыли/воды, Комплектация...</td>\n",
       "      <td>False</td>\n",
       "      <td>516c4c0cca619ea4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>a99c9cc7e081c1f2</td>\n",
       "      <td>Мобильный телефон Samsung Galaxy Note 20 Ultra...</td>\n",
       "      <td>[Операционная   система\\tAndroid 10, SIM-карты...</td>\n",
       "      <td>False</td>\n",
       "      <td>7c078e4143695811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>d9fe709dfef4b09d</td>\n",
       "      <td>Роутер беспроводной Mercusys MW301R N300 10/10...</td>\n",
       "      <td>[Подключение  к интернету (WAN)\\tEthernet RJ-4...</td>\n",
       "      <td>False</td>\n",
       "      <td>21991eb9be11bcfe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            product_id                                               name  \\\n",
       "991   4ce12b22a6dfed7a                 Mi Robot Vacuum-Mop 2 Pro EU White   \n",
       "2146  a99c9cc7e081c1f2  Мобильный телефон Samsung Galaxy Note 20 Ultra...   \n",
       "2772  d9fe709dfef4b09d  Роутер беспроводной Mercusys MW301R N300 10/10...   \n",
       "\n",
       "                                                  props  is_reference  \\\n",
       "991   [Тип   контейнера\\tдля пыли/воды, Комплектация...         False   \n",
       "2146  [Операционная   система\\tAndroid 10, SIM-карты...         False   \n",
       "2772  [Подключение  к интернету (WAN)\\tEthernet RJ-4...         False   \n",
       "\n",
       "          reference_id  \n",
       "991   516c4c0cca619ea4  \n",
       "2146  7c078e4143695811  \n",
       "2772  21991eb9be11bcfe  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(df):\n",
    "    \n",
    "    # т.к. описание в формате list с несколькими данными объединим в один список \n",
    "\n",
    "    def jn(x):\n",
    "        x = \" \".join([ch for ch in x])\n",
    "        x = str(x)\n",
    "        return x\n",
    "\n",
    "    df['props_un'] = df['props'].apply(jn)\n",
    "\n",
    "    # добавим имя продукта для обработки\n",
    "\n",
    "    def jn_name(x):\n",
    "        x = \"\".join([ch for ch in x])\n",
    "        x = str(x)\n",
    "        return x\n",
    "\n",
    "    df['props_un'] = df['props_un']+ ' ' +df['name'].apply(jn_name)\n",
    "\n",
    "    # избавляемся от табуляции\n",
    "\n",
    "    def rem_tab(x):\n",
    "        x = x.replace(\"\\t\", \" \")\n",
    "        return x\n",
    "\n",
    "    df['props_un'] = df['props_un'].progress_apply(rem_tab)\n",
    "\n",
    "    # избавляемся от знаков препинания\n",
    "\n",
    "    spec_chars = string.punctuation + '«'+ '»'+ '—'+ '\"'+ '\"'\n",
    "    print(spec_chars)\n",
    "\n",
    "    def rem_spec_chars(x):\n",
    "        x = \"\".join([ch for ch in x if ch not in spec_chars])\n",
    "        x = \"\".join([x.replace('\\d+', '')])\n",
    "        return x\n",
    "\n",
    "    df['props_un'] = df['props_un'].progress_apply(rem_spec_chars)\n",
    "\n",
    "    # переводим всё в нижний регистр\n",
    "\n",
    "    def low(x):\n",
    "        x = list(x.split())\n",
    "        x = [w.lower() for w in x]\n",
    "        return x\n",
    "\n",
    "    df['props_un'] = df['props_un'].progress_apply(low)\n",
    "\n",
    "    # удаляем стоп слова\n",
    "\n",
    "    def stop_words(x):\n",
    "        new_x = []\n",
    "        for w in x:\n",
    "            if w not in russian_stopwords:\n",
    "                new_x.append(w)\n",
    "        return new_x\n",
    "\n",
    "    df['props_un'] = df['props_un'].progress_apply(stop_words)\n",
    "\n",
    "    # лемматизируем текст\n",
    "\n",
    "    def lem(x):\n",
    "        #x = list(x.split())\n",
    "        x = [morph.parse(w)[0].normal_form for w in x]\n",
    "        return x\n",
    "\n",
    "    df['props_un'] = df['props_un'].progress_apply(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3251/3251 [00:00<00:00, 239520.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~«»—\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3251/3251 [00:00<00:00, 16991.38it/s]\n",
      "100%|██████████| 3251/3251 [00:00<00:00, 49492.69it/s]\n",
      "100%|██████████| 3251/3251 [00:00<00:00, 10290.72it/s]\n",
      "100%|██████████| 3251/3251 [00:24<00:00, 133.67it/s]\n"
     ]
    }
   ],
   "source": [
    "prep(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3251/3251 [00:00<00:00, 11993.56it/s]\n"
     ]
    }
   ],
   "source": [
    "unique_words = {}\n",
    "for i in tqdm(df.index):\n",
    "    for j in set(df['props_un'].loc[i]):\n",
    "        if len(j) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            if j in unique_words.keys():\n",
    "                unique_words[j] += 1\n",
    "            else:\n",
    "                unique_words[j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4160/4160 [00:00<00:00, 539393.61it/s]\n"
     ]
    }
   ],
   "source": [
    "pop_words = []\n",
    "for i in tqdm(unique_words.keys()):\n",
    "    if unique_words[i] < 4:\n",
    "        pass\n",
    "    elif unique_words[i] > len(unique_words)*0.9:\n",
    "        pass\n",
    "    else:\n",
    "        pop_words.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Максимальное количество слов \n",
    "num_words = len(pop_words)\n",
    "# Максимальная длина новости\n",
    "max_news_len = 67\n",
    "# Количество классов новостей\n",
    "nb_classes = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделим трейн и тест\n",
    "\n",
    "X = df['props_un']\n",
    "y = df['reference_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# делаем токенизатор на списке всех слов и сохраним его в отдельном файле\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "tokenizer.fit_on_texts(df['props_un'])\n",
    "\n",
    "with open(\"./output/tokinaizer.json\", \"w\") as outfile:\n",
    "    json.dump(tokenizer.word_index, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизируем трейн и тест\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "x_train = pad_sequences(train_sequences, maxlen=max_news_len)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "x_test = pad_sequences(test_sequences, maxlen=max_news_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сверточная нейронная сеть\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(num_words, 128, input_length=max_news_len))\n",
    "model_cnn.add(Conv1D(1024, 5, padding='valid', activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(512, activation='relu'))\n",
    "model_cnn.add(Dense(471, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 28s 686ms/step - loss: 6.1360 - accuracy: 0.0181 - val_loss: 6.0929 - val_accuracy: 0.0192\n"
     ]
    }
   ],
   "source": [
    "history_cnn = model_cnn.fit(x_train, \n",
    "                            y_train, \n",
    "                            epochs=1,\n",
    "                            batch_size=64,\n",
    "                            validation_split=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DIR + 'agora_hack_products/test_request.json', encoding='utf-8') as f:\n",
    "   tst = json.load(f)\n",
    "\n",
    "test = pd.DataFrame.from_dict(tst, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 199919.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~«»—\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 18064.10it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 98968.95it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 7322.27it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 132.53it/s]\n"
     ]
    }
   ],
   "source": [
    "prep(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_sequences = tokenizer.texts_to_sequences(test['props'])\n",
    "\n",
    "x_test_2 = pad_sequences(test_2_sequences, maxlen=max_news_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`load_weights` requires h5py package when loading weights from HDF5. Try installing h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\AgoraHack\\agorahack-case1-team0\\main_notebook.ipynb Ячейка 23\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/AgoraHack/agorahack-case1-team0/main_notebook.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_cnn\u001b[39m.\u001b[39;49mload_weights(\u001b[39m'\u001b[39;49m\u001b[39m./output/best_model_cnn.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Rusla\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Rusla\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2640\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2638\u001b[0m status \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2639\u001b[0m \u001b[39mif\u001b[39;00m h5py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2640\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   2641\u001b[0m       \u001b[39m'\u001b[39m\u001b[39m`load_weights` requires h5py package when loading weights from \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2642\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mHDF5. Try installing h5py.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2643\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_graph_network \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m   2644\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2645\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mUnable to load weights saved in HDF5 format into a subclassed \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2646\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mModel which has not created its variables yet. Call the Model \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2647\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mfirst, then load the weights.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: `load_weights` requires h5py package when loading weights from HDF5. Try installing h5py."
     ]
    }
   ],
   "source": [
    "model_cnn.load_weights('./output/best_model_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11fd67006ca27d8c3de757c93fc3ec30e6504c490ec43f275b6f9d00aa4c6782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
